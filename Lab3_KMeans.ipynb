{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-scale Distributed $k$-means Algorithm\n",
    "\n",
    "In this Notebook, we'll focus on the development of a simple distributed algorithm. As for the Notebook on SGD, we focus on iterative algorithms, which eventually converge to a desired solution.\n",
    "\n",
    "In what follows, we'll proceed with the following steps:\n",
    "\n",
    "* We first introduce formally the $k$-means algorithm\n",
    "* Then we focus on a serial implementation. To do this, we'll first generate some data using scikit. In passing, we'll also use the $k$-means implementation in scikit to have a baseline to compare against.\n",
    "* Subsequently, we will focus on some important considerations and improvements to the serial implementation of $k$-means.\n",
    "* At this point, we'll design our distributed version of the $k$-means algorithm using pyspark, and re-implement the enhancements we designed for the serial version \n",
    "\n",
    "#### References:\n",
    "* https://en.wikipedia.org/wiki/K-means_clustering\n",
    "* http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/clustering/kmeans.ipynb\n",
    "* https://apache.googlesource.com/spark/+/master/examples/src/main/python/kmeans.py\n",
    "* https://github.com/castanan/w2v/blob/master/ml-scripts/w2vAndKmeans.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preamble code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries: the $k$-means algorithm\n",
    "\n",
    "$k$-means clustering aims to partition $n$ $d-$dimensional observations into $k$ clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells.\n",
    "The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. In this Notebook we'll focus on one of them in particular: the Lloyd algorithm.\n",
    "\n",
    "The $k$-means problem can be formalized as follows. Given a set of observations $(x_1, x_2, \\cdots, x_n)$, where each observation is a $d$-dimensional real vector, $k$-means clustering aims to partition the $n$ observations into $k \\leq n$ sets $S = \\{S_1, S_2, \\cdots, S_k\\}$ so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). \n",
    "The objective is to find:\n",
    "\n",
    "$$\n",
    "\\arg \\min_S \\sum_{i=1}^{k} \\sum_{x \\in S_i} || \\boldsymbol{x} - \\boldsymbol{\\mu_i} ||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common algorithm uses an iterative refinement technique. Given an initial set of $k$ centroids $\\mu_1^{(1)}, \\cdots, \\mu_k^{(1)}$ , the algorithm proceeds by alternating between two steps: in the **assignment step**, observations are associated to the closest **centroid**, in terms of squared Euclidean distance; in the **update step** new centroids are computed based on the new points associated to each centroid. Note: $\\mu_i^{(t)}$ stands for the $i$-th centroid as of the $t$-th iteration. So $\\mu_1^{(1)}$ is the centroid 1 at iteration 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm pseudo-code\n",
    "It is important to work on a principled approach to the design of large-scale algorithms, and this starts with using good data structures and scientific libraries, such as ```numpy``` and ```scipy```. In particular, we will focus on the use of ```numpy``` arrays, which come with efficient methods for array operations. A pseudo-code for the $k$-means algorithm is specified below:\n",
    "\n",
    "```python\n",
    "def kmeans(X, k, maxiter, seed=None):\n",
    "    \"\"\"\n",
    "    specify the number of clusters k and\n",
    "    the maximum iteration to run the algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    # randomly choose k data points as initial centroids\n",
    "    centroids = X[rand_indices]\n",
    "    \n",
    "    for itr in range(maxiter):\n",
    "        # ---------------\n",
    "        # ASSIGNMENT STEP\n",
    "        # ---------------\n",
    "        # compute the distance matrix between each data point and the set of centroids\n",
    "        distance_matrix = # row Index = data point Index; col Index = centroid Index; value=distance\n",
    "        # assign each data point to the closest centroid\n",
    "        cluster_assignment = # array Index = data point Index; array value = closest centroid Index\n",
    "        \n",
    "        # UPDATE STEP\n",
    "        # select all data points that belong to cluster i and compute\n",
    "        # the mean of these data points (each feature individually)\n",
    "        # this will be our new cluster centroids\n",
    "        new_centroids = ...\n",
    "        \n",
    "        # STOP CONDITION\n",
    "        # if centroids == new_centroids => stop\n",
    " \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data generation: working in bi-dimensional spaces\n",
    "\n",
    "Next, we use sklearn to generate some synthetic data to test our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples = 300, centers = 4,\n",
    "                  random_state = 0, cluster_std = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change default figure and font size\n",
    "plt.rcParams['figure.figsize'] = 8, 6 \n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# scatter plot\n",
    "plt.scatter(X[:, 0], X[:, 1], s = 50)\n",
    "plt.ylim(-2, 10)\n",
    "plt.xlim(-6, 6)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question 1.</strong> Implement your own version of k-means, as a serial algorithm.\n",
    "\n",
    "Follow the guidelines below:\n",
    "\n",
    "<ul>\n",
    "    <li> Define a function to perform k-means clustering. The function should accept as inputs: the training data x, the number of clusters k, and the iteration budget you allocate to the algorithm. Additional arguments might include the use of a random seed to initialize centroids.</li>\n",
    "    <li>The function should output the centroids, and the cluster assignment, that is, to which centroid each data point is assigned to</li>\n",
    "    <li> Optionally, keep track of the position of the centroids, for each iteration.</li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "Once the ```kmeans``` function is defined, you can generate input data according to the cell above, that uses scikitlearn.\n",
    "<br>\n",
    "\n",
    "The output of your cell should contain the following information:\n",
    "<ul>\n",
    "    <li> Print the number of data points that belong to each cluster</li>\n",
    "    <li> Plot the clustered data points:</li>\n",
    "    <ul>\n",
    "        <li>Using different colors for each cluster</li>\n",
    "        <li>Plot the centroid positions for each cluster</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question 2.</strong> Use the built-in k-means implementation in sklearn and determine centroids and clusters.\n",
    "\n",
    "Follow the guidelines below:\n",
    "\n",
    "<ul>\n",
    "    <li> Use the KMeans algorithm from sklearn</li>\n",
    "    <li> Use the fit_predict method to cluster data</li>\n",
    "    <li> Use the cluster_centers_ method to retrieve centroids</li>\n",
    "</ul>\n",
    "\n",
    "The output of your cell should contain the following information:\n",
    "<ul>\n",
    "    <li> Plot the clustered data points, using the same code your have produced for Question.1</li>\n",
    "    <ul>\n",
    "        <li>Using different colors for each cluster</li>\n",
    "        <li>Plot the centroid positions for each cluster</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question 3.</strong> Use the sklearn dataset API to generate alternative synthetic data to test your k-means algorithm implementation.\n",
    "\n",
    "Follow the guidelines from this document: http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\n",
    "\n",
    "\n",
    "The output of your cell should contain the following information:\n",
    "<ul>\n",
    "    <li> Plot the new synthetic dataset you generated\n",
    "    <li> Plot the clustered data points, using the same code your have produced for Question.1</li>\n",
    "    <ul>\n",
    "        <li>Using different colors for each cluster</li>\n",
    "        <li>Plot the centroid positions for each cluster</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simplified analysis of algorithm convergence\n",
    "\n",
    "One well-known weakness of $k$-means is that the algorithm's performance is closely tied with the randomly generated initial centroids' quality. If the algorithm starts with a set of bad inital centers, it will get stuck in a local minimum.\n",
    "\n",
    "Instead of taking a formal approach to study the convergence of $k$-means, let's study it with an experimental approach. One thing we can do is to build a measure of clustering quality: intuitively, a good clustering result should produce clusters in which data points should be very close to their centroids, and very far from other centroids. In this Notebook, we'll look at a metric called the **total within Sum of Squares**, which is sometimes referred ot as heterogeneity. Mathematically, we define heterogeneity as:\n",
    "\n",
    "$$\n",
    "\\mathcal{H} = \\sum_{j=1}^{k} \\sum_{i: z_i=j} || \\boldsymbol{x_i} - \\boldsymbol{\\mu_j}||_{2}^{2}\n",
    "$$\n",
    "\n",
    "Where $k$ denotes the total number of clusters, $x_i$ is the $i$-th data point, $\\mu_j$ is the $j$-th centroid, and $|| \\cdot ||_{2}^{2}$ denotes the squared L2 norm (Euclidean distance) between the two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question 4.</strong> Modify your own version of k-means, to compute heterogeneity as defined above.\n",
    "\n",
    "Follow the guidelines below:\n",
    "\n",
    "<ul>\n",
    "    <li>Use the same method template you used in Question 1</li>\n",
    "    <li>Add the code required to compute heterogeneity</li>\n",
    "    <li>The function should return, in addition to the same return values as for the baseline version, the computed heterogeneity\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question 5.</strong> Using the modified k-means method you designed, study algorithm convergence as a function of heterogeneity.\n",
    "\n",
    "Follow the guidelines below:\n",
    "\n",
    "<ul>\n",
    "    <li>Run the modified k-means for at least 5 different initial seed values</li>\n",
    "    <li>Prepare a dictionary data structure containing: key = random seed, value = heterogeneity</li>\n",
    "    <li>Print seed, heterogeneity values</li>\n",
    "</ul>\n",
    "\n",
    "Add your personal comment about the convergence properties of the $k$-means algorithm.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A technique for a smart centroid initialization: $k$-means++\n",
    "\n",
    "One effective way to produce good initial centroids to feed to $k$-means is to proceed as follows: instead of randomly generating initial centroids, we will try to spread them out in our $d$-dimensional space, such that they are not \"too close\" to eachother. If you are interested in the details of this technique, you should refer to the link to the original research paper below: in summary, the $k$-means++ technique allows to improve the quality of the local optima in which $k$-means settles, and to reduce the average runtime of the algorithm.\n",
    "\n",
    "k-means++: the advantages of careful seeding, by David Arthur and Sergei Vassilvitskii (Stanford University) https://dl.acm.org/citation.cfm?id=1283494\n",
    "\n",
    "A simplified workflow of the $k$-means++ approach is as follows:\n",
    "\n",
    "\n",
    "* Choose a data point at random from the dataset, this serves as the first centroid\n",
    "* Compute the squared euclidean distance of all other data points to the randomly chosen first centroid\n",
    "* To generate the next centroid, each data point is chosen with the probability (weight) of its squared distance to the chosen center in the current round, divided by the the total squared distance (this is just a normalization to make sure the probability adds up to 1). In other words, a new centroid should be as far as possible from the other centroids\n",
    "* Next, recompute the probability (weight) of each data point as the minimum of the distance between it and all the centers that are already generated (e.g. for the second iteration, compare the data point's distance between the first and second center and choose the smaller one)\n",
    "* Repeat step 3 and 4 until we have $k$ initial centroids to feed to the $k$-means algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question 6.</strong> Modify your own version of k-means, to introduce the smart initialization technique described above. Don't forget to keep track of heterogeneity as well! The whole point is to measure if k-means++ really improves on this metric.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Follow the guidelines below:\n",
    "\n",
    "<ul>\n",
    "    <li>Use the same method template you used in Question 1</li>\n",
    "    <li>Add the code required to compute the initial clusters according to k-means++</li>\n",
    "    <li>Add the code required to compute heterogeneity</li>\n",
    "    <li>The function should return, in addition to the same return values as for the baseline version, the computed heterogeneity\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question 7.</strong> Similarly to question 5, using the modified k-means method you designed, study algorithm convergence as a function of heterogeneity.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "Follow the guidelines below:\n",
    "\n",
    "<ul>\n",
    "    <li>Run the modified k-means for at least 5 different initial seed values</li>\n",
    "    <li>Prepare a dictionary data structure containing: key = random seed, value = heterogeneity</li>\n",
    "    <li>Print seed, heterogeneity values</li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "\n",
    "One additional question to answer is the following: print the average heterogeneity for the baseline k-means algorithm, and the average heterogeneity when using the k-means++ initialization. Compare and comment with your own words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the value of k: a simple and visual approach, called the Elbow method\n",
    "\n",
    "Another problem of $k$-means is that we have to specify the number of clusters $k$ before running the algorithm, which we often don't know a priori. \n",
    "There are many different heuristics for choosing a suitable value for $k$, the simplest one being the **Elbow method**. Essentially, the idea is to run the $k$-means algorithm using different values of $k$ and plot the corresponding heterogeneity. This measure will decrease as the number of clusters increases, because each cluster will be smaller and tighter. \n",
    "By visual inspection of the plot heterogeneity vs. $k$, we will (hopefully!) see that the curve flattens out at some value of $k$: this is what we call an \"elbow\", and we'll select the value of $k$ corresponding to the \"elbow\" position.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question 8.</strong> Using the k-means algorithm you implemented, including the smart initialization technique, collect into an array the value of heterogeneity as a function of the number of clusters k, where k is to be selected in the range [2,10].\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "Your output cell should contain:\n",
    "\n",
    "<ul>\n",
    "    <li>The plot of heterogeneity vs. k</li>\n",
    "    <li>A discussion on your visual inspection of the curve, together with a justification for an appropriate choice of the value k</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed $k$-means with PySpark\n",
    "\n",
    "We're now ready to work on a distributed implmentation of the $k$-means algorithm, using the PySpark API.\n",
    "\n",
    "By now, you should be rather familiar with the $k$-means algorithm, which means we can focus on its parallel and distributed design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed algorithm pseudo code\n",
    "\n",
    "The basic idea of distributed $k$-means is as follows: data points to be clustered should be stored as a distributed dataset, namely a RDD. As in the Notebook on distributed SGD, we will take a shortcut and avoid using HDFS RDDs: rather, we'll use sklearn to generate the data points, similary to the serial version of the algorithms, then use the ```parallelize()``` method to create an RDD, and determine the number of partitions.\n",
    "\n",
    "We also need to manipulate the centroids array: indeed, all machines should hold a copy of the centroid vector, such that they can proceed independently and in parallel in the first phase of the $k$-means algorithm, that is the **assignment step**. Specifically, every worker has a set of data points, and it will use a copy of the centroid vector to compute cluster assignement: we compute the distance between each data point and each centroid, to assign data points to their closest centroid.\n",
    "\n",
    "Once the assignement step is done, we need to recompute new centroids based on the assignement, that is, we execute the **update step**. Clearly, we will need to **shuffle** data over the network such that we will have, for each current centroid, the list of all data points that have been assigned to it. If you think about it, this problem should be familiar!! This is very similar to what we do in the Word Count example. As such, you will need to make sure the output of the update step is cast to a ```<key, value>``` type, where the key corresponds to a centroid identifier, and the value contains the list of data points associated to that centroid. The framework will take care of the distributed group by operation, and organize data according to the semantic of our algorithm.\n",
    "\n",
    "**NOTE:** since we will (potentially) work on large dataset sizes, we don't want our algorithm to return the final assignement after convergence, for otherwise we would need to collect a large amount of data in the driver machine, which has a finite and somehow limited amount of RAM.\n",
    "\n",
    "The pseudo code of the algorithm you need to implement is as follows:\n",
    "\n",
    "```python\n",
    "datapoints = # Use sklearn, as usual, and work on blobs\n",
    "centroids = # Random initialization\n",
    "\n",
    "for itr in range(maxiter): # This for loop is executed by the driver\n",
    "    bcCentroids = sc.broadcast(centroids) # Use broadcast variables\n",
    "    \n",
    "    closest = datapoints.mapPartition(assignement_step) # This should happen in parallel\n",
    "    \n",
    "    centroids = closest.reduceByKey(update_step_sum). \\ # This should happen in parallel\n",
    "        map(update_step_mean). \\ # This should happen in parallel\n",
    "        collect() # Here we collect new centroids in the driver\n",
    "```\n",
    "\n",
    "As you can see from the pseudo code, you need to figure out how to implement the ```assignement_step``` function and the update_step function. For the latter, the pseudo code gives you a big hint! Remember what we discussed in class about computing the mean!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question 9.</strong> Implement the distributed version of the k-means algorithm, following the guidelines in the pseudo code.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Your output cell should contain:\n",
    "\n",
    "<ul>\n",
    "    <li>The value of the centroids once the algorithm converges</li>\n",
    "    <li>The total runtime of the distributed algorithm, in seconds</li>\n",
    "    <li>A visualization of the data points and the computed centroids</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question 10.</strong> Answer the following questions:\n",
    "<br>\n",
    "<br>\n",
    "<ul>\n",
    "    <li>How many partitions did you use? Why?</li>\n",
    "    <li>What is the size of the dataset you generate? Did you cache the dataset? What's the RAM occupation?</li>\n",
    "    <li>What is the size of the shuffle data over the network? How does it compare to the dataset size?</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question 11.</strong> Comparison between serial and distributed implementations.\n",
    "<br>\n",
    "<br>\n",
    "Given the dataset size you chose for your experiments, answer the following questions:\n",
    "<ul>\n",
    "    <li>Which is \"faster\", the serial or distributed implementation of k-means?</li>\n",
    "    <li>What is a dataset size for which the distributed implementation is clearly faster than the serial one?</li>\n",
    "    <li>What would be different in your code, should the input dataset reside on disk? Clearly, the input RDD would be reading from HDFS. Any other differences with respect to partitions?</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
